import os

import numpy as np
import torch
import torchvision.transforms as transforms
from PIL import Image
from torch.autograd import Variable
from torchvision.utils import save_image

from utils.load_custom_dataset import ImageLoader
from config.config_core import cuda, FloatTensor
from config.config_params_v3 import Opt
from utils import load_image_dataset
from utils.helper import to_categorical, crop, min_max_scaler


def fix_image(img: Image, output_path: str):
    img_width, img_height = img.size
    if img_width < img_height:
        ratio = int(img_height / img_width)
        box = (0, 0, img_height - img_width * ratio, img_height)
        added_img = img.crop(box)
        added_img_val = np.asarray(added_img)
        img_val = np.asarray(img)
        img_val_final = img_val
        if ratio > 1:
            for _ in range(1, ratio):
                img_val_final = np.concatenate((img_val_final, img_val), axis=1)
        img_val_final = np.concatenate((img_val_final, added_img_val), axis=1)
        merged_img = Image.fromarray(img_val_final)
        # merged_img.show()
        merged_img.save(output_path)
    else:
        img.save(output_path)
        print(f"{output_path} - {img_width}, {img_height}")


def generate_fixed_images(ver):
    print("Only run it after get dataset once")
    PATH_TO_GET_DATA = f"../data/dataset-v{ver}/data"
    folders = os.listdir(PATH_TO_GET_DATA)
    print(folders)

    for folder in folders:
        out_folder = f"../data/dataset-v{ver}/fixed-data/{folder}"
        os.makedirs(out_folder, exist_ok=True)
        files = os.listdir(f"{PATH_TO_GET_DATA}/{folder}")
        print(files)
        for file in files:
            image = Image.open(f"{PATH_TO_GET_DATA}/{folder}/{file}")
            out_path = f"{out_folder}/{file}"
            fix_image(img=image, output_path=out_path)


def generate_sample_image(n_row, generator, static_label, static_code, latent_dim=None, path=None):
    """Saves a grid of generated digits ranging from 0 to n_classes"""
    # Static sample
    latent_space = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, latent_dim))))
    static_sample = generator(latent_space, static_label, static_code)
    save_image(static_sample.data, path, nrow=n_row, normalize=True)


def generate_code_input(opt: Opt, size_image, path_get_features=None, path_get_data=None):
    if size_image == 32:
        transform = transforms.Compose(
            [transforms.Resize(opt.img_size_32),
             transforms.CenterCrop(opt.img_size_32),
             transforms.ToTensor(),
             transforms.Grayscale(),
             transforms.Normalize([0.5], [0.5])]
        )
    elif size_image == 64:
        transform = transforms.Compose(
            [transforms.Resize(opt.img_size_64),
             transforms.CenterCrop(opt.img_size_64),
             transforms.ToTensor(),
             transforms.Grayscale(),
             transforms.Normalize([0.5], [0.5])]
        )
    else:
        transform = transforms.Compose(
            [transforms.Resize(opt.img_size_128),
             transforms.CenterCrop(opt.img_size_128),
             transforms.ToTensor(),
             transforms.Grayscale(),
             transforms.Normalize([0.5], [0.5])]
        )

    data_read = ImageLoader(annotations_file=path_get_features, root=path_get_data, transform=transform)
    dll_per_label = []
    cs_per_label = []
    for name_class in data_read.classes:
        filtered_sample = data_read.pe_features[data_read.pe_features['legitimate'].str.contains(name_class)]
        dll_per_label.append(filtered_sample["DllCharacteristics"].unique())
        cs_per_label.append(filtered_sample["CheckSum"].unique())

    unique_cs_vals = data_read.pe_features["CheckSum"].unique()
    unique_dll_vals = data_read.pe_features["DllCharacteristics"].unique()
    feature_code = np.zeros((opt.n_classes ** 2, opt.code_dim))
    scaled_cs_vals = min_max_scaler(unique_cs_vals, -2, 2)
    label_id = 0
    for idx in range(feature_code.shape[0]):
        dll_value = np.random.choice(dll_per_label[label_id])
        checksum_value = np.random.choice(cs_per_label[label_id])
        checksum_val_id = np.where(unique_cs_vals == checksum_value)[0]
        for index in range(len(unique_dll_vals)):
            if unique_dll_vals[index] == dll_value:
                feature_code[idx, index] = 1.0
                feature_code[idx, -1] = scaled_cs_vals[checksum_val_id]
                break

        label_id += 1
        if label_id == 7:
            label_id = 0

    return Variable(FloatTensor(feature_code))


def generate_fake_data(opt: Opt, size_image: int, ver: str):
    os.makedirs(f"../data/dataset-v{ver}/fake_data_{size_image}/Babuk_0/", exist_ok=True)
    os.makedirs(f"../data/dataset-v{ver}/fake_data_{size_image}/Cerber_1/", exist_ok=True)
    os.makedirs(f"../data/dataset-v{ver}/fake_data_{size_image}/Conti_2/", exist_ok=True)
    os.makedirs(f"../data/dataset-v{ver}/fake_data_{size_image}/GandCrab_3/", exist_ok=True)
    os.makedirs(f"../data/dataset-v{ver}/fake_data_{size_image}/Nitro_4/", exist_ok=True)
    os.makedirs(f"../data/dataset-v{ver}/fake_data_{size_image}/Ryuk_5/", exist_ok=True)
    os.makedirs(f"../data/dataset-v{ver}/fake_data_{size_image}/Stop_6/", exist_ok=True)
    os.makedirs(f"../data/dataset-v{ver}/fake_data_{size_image}/WannaCry_7/", exist_ok=True)
    os.makedirs(f"../data/dataset-v{ver}/generated_data/", exist_ok=True)
    PATH_GET_PE_FEATURES = f"../data/dataset-v{ver}/classification_ransomware_family.xlsx"
    PATH_GET_DATA = f"../data/dataset-v{ver}/fixed-data"

    if size_image == 32:
        from models.info_gan_original import Generator
        generator_path = "weights/generator_V3_600_32px.pt"
    elif size_image == 64:
        from models.info_gan_v64 import Generator
        generator_path = "weights/generator_V3_600_64px.pt"
    else:
        from models.info_gan_v128 import Generator
        generator_path = "weights/generator_V3_bat_14591_128px.pt"

    generator_load = Generator(opt=opt)
    if cuda:
        print("We are using GPU for generate fake data")
        generator_load.cuda()
    generator_load.load_state_dict(torch.load(generator_path))
    generator_load.eval()

    count_img = 0
    for index in range(13):
        print(index)
        static_label = to_categorical(
            y=np.array([num for _ in range(opt.n_classes) for num in range(opt.n_classes)]),
            num_columns=opt.n_classes,
            FloatTensor=FloatTensor
        )
        static_code = generate_code_input(opt=opt, size_image=size_image, path_get_features=PATH_GET_PE_FEATURES,
                                          path_get_data=PATH_GET_DATA)
        # static_code = Variable(FloatTensor(np.zeros((opt.n_classes ** 2, opt.code_dim))))
        path_fake_data = f"../data/dataset-v{ver}/generated_data/{index}.png"
        generate_sample_image(n_row=opt.n_classes, generator=generator_load, static_label=static_label,
                              static_code=static_code, latent_dim=opt.latent_dim, path=path_fake_data)
        path_output = f"../data/dataset-v{ver}/fake_data_{size_image}"
        crop(path_input=path_fake_data, path_output=path_output, heightCut=size_image, widthCut=size_image, border=2,
             count=count_img)
        count_img += 1


def generate_real_data(opt: Opt, size_img: int, ver: str):
    os.makedirs(f"../data/dataset-v{ver}/fake_data_{size_img}/Babuk/", exist_ok=True)
    os.makedirs(f"../data/dataset-v{ver}/fake_data_{size_img}/Cerber/", exist_ok=True)
    os.makedirs(f"../data/dataset-v{ver}/fake_data_{size_img}/Conti/", exist_ok=True)
    os.makedirs(f"../data/dataset-v{ver}/fake_data_{size_img}/GandCrab/", exist_ok=True)
    os.makedirs(f"../data/dataset-v{ver}/fake_data_{size_img}/Nitro/", exist_ok=True)
    os.makedirs(f"../data/dataset-v{ver}/fake_data_{size_img}/Ryuk/", exist_ok=True)
    os.makedirs(f"../data/dataset-v{ver}/fake_data_{size_img}/Stop/", exist_ok=True)
    os.makedirs(f"../data/dataset-v{ver}/fake_data_{size_img}/WannaCry/", exist_ok=True)
    if size_img == 32:
        transform = transforms.Compose(
            [transforms.Resize(opt.img_size_32),
             transforms.CenterCrop(opt.img_size_32),
             transforms.ToTensor(),
             transforms.Grayscale(),
             transforms.Normalize([0.5], [0.5])]
        )
    elif size_img == 64:
        transform = transforms.Compose(
            [transforms.Resize(opt.img_size_64),
             transforms.CenterCrop(opt.img_size_64),
             transforms.ToTensor(),
             transforms.Grayscale(),
             transforms.Normalize([0.5], [0.5])]
        )
    else:
        transform = transforms.Compose(
            [transforms.Resize(opt.img_size_128),
             transforms.CenterCrop(opt.img_size_128),
             transforms.ToTensor(),
             transforms.Grayscale(),
             transforms.Normalize([0.5], [0.5])]
        )
    PATH_TO_GET_DATA = f"../data/dataset-v{ver}/fixed-data"

    data_read = load_image_dataset.read_dataset(path=PATH_TO_GET_DATA, transform_info=transform)
    for idx, data in enumerate(data_read):
        # print(idx, data)
        label_index = data[1]
        if label_index == 0:
            save_image(data[0], f'../data/dataset-v{ver}/fake_data_{size_img}/Babuk/Babuk_{idx}.png', normalize=True)
        elif label_index == 1:
            save_image(data[0], f'../data/dataset-v{ver}/fake_data_{size_img}/Cerber/Cerber_{idx}.png', normalize=True)
        elif label_index == 2:
            save_image(data[0], f'../data/dataset-v{ver}/fake_data_{size_img}/Conti/Conti_{idx}.png', normalize=True)
        elif label_index == 3:
            save_image(data[0], f'../data/dataset-v{ver}/fake_data_{size_img}/GandCrab/BandCrab_{idx}.png', normalize=True)
        elif label_index == 4:
            save_image(data[0], f'../data/dataset-v{ver}/fake_data_{size_img}/Nitro/Nitro_{idx}.png', normalize=True)
        elif label_index == 5:
            save_image(data[0], f'../data/dataset-v{ver}/fake_data_{size_img}/Ryuk/Ryuk_{idx}.png', normalize=True)
        elif label_index == 6:
            save_image(data[0], f'../data/dataset-v{ver}/fake_data_{size_img}/Stop/Stop_{idx}.png', normalize=True)
        elif label_index == 7:
            save_image(data[0], f'../data/dataset-v{ver}/fake_data_{size_img}/WannaCry/WannaCry_{idx}.png', normalize=True)


if __name__ == '__main__':
    opt = Opt()
    version = "2.1"
    # generate_fixed_images(ver=version)
    for size_image in [32, 64, 128]:
        generate_fake_data(opt=opt, size_image=size_image, ver=version)
        generate_real_data(opt=opt, size_img=size_image, ver=version)
