import os

import numpy as np
import torch
from torch.autograd import Variable
from torchvision.utils import save_image
import torchvision.transforms as transforms

from utils import load_image_dataset
from config.config_core import cuda, FloatTensor, LongTensor
from config.config_params_v2 import opt_v2
from config.config_params import opt
from models.info_gan_original import Generator
from models.info_gan_original import Discriminator
from utils.helper import to_categorical, sample_image, crop


def generate_sample_image(n_row, generator, static_label, static_code, latent_dim=None,
                          FloatTensor=None, path=None):
    """Saves a grid of generated digits ranging from 0 to n_classes"""
    # Static sample
    latent_space = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, latent_dim))))
    static_sample = generator(latent_space, static_label, static_code)
    save_image(static_sample.data, path, nrow=n_row, normalize=True)


def generate_fake_data():
    os.makedirs("../data/dataset-v1.1/fake_data/Stop_0/", exist_ok=True)
    os.makedirs("../data/dataset-v1.1/fake_data/GandCrap_1/", exist_ok=True)
    os.makedirs("../data/dataset-v1.1/fake_data/Nitro_2/", exist_ok=True)
    os.makedirs("../data/dataset-v1.1/generated_data/", exist_ok=True)
    generator_load = Generator(opt=opt_v2)
    discriminator_load = Discriminator(opt=opt_v2)
    if cuda:
        print("We are using GPU for generate fake data")
        generator_load.cuda()
        discriminator_load.cuda()

    generator_load.load_state_dict(torch.load('weights/generator_V1.pt'))
    generator_load.eval()
    discriminator_load.load_state_dict(torch.load('weights/discriminator_V1.pt'))
    discriminator_load.eval()

    count_img = 0
    for index in range(50):
        print(index)
        static_label = to_categorical(
            y=np.array([num for _ in range(opt_v2.n_classes) for num in range(opt_v2.n_classes)]), num_columns=opt_v2.n_classes,
            FloatTensor=FloatTensor
        )
        static_code = Variable(FloatTensor(np.zeros((opt_v2.n_classes ** 2, opt_v2.code_dim))))
        path_fake_data = f"../data/dataset-v1.1/generated_data/{index}.png"
        # generate_sample_image(n_row=3, generator=generator_load, static_label=static_label,
        #                       static_code=static_code, FloatTensor=FloatTensor, latent_dim=opt_v2.latent_dim,
        #                       path=path_fake_data)
        path_output = "../data/dataset-v1.1/fake_data"
        crop(path_input=path_fake_data, path_output=path_output, heightCut=32, widthCut=32, border=2, count=count_img)
        count_img += 1


def generate_real_data_32px():
    os.makedirs("../data/dataset-v1.1/fake_data/Stop/", exist_ok=True)
    os.makedirs("../data/dataset-v1.1/fake_data/GandCrap/", exist_ok=True)
    os.makedirs("../data/dataset-v1.1/fake_data/Nitro/", exist_ok=True)
    PATH_TO_GET_DATA = "../data/dataset-v1.1/data"
    transform = transforms.Compose(
        [transforms.Resize(opt.img_size),
         transforms.CenterCrop(opt.img_size),
         transforms.ToTensor(),
         transforms.Grayscale(),
         transforms.Normalize([0.5], [0.5])]
    )

    data_read = load_image_dataset.read_dataset(path=PATH_TO_GET_DATA, transform_info=transform)
    for idx, data in enumerate(data_read):
        print(idx, data)
        if data[1] == 0:
            save_image(data[0], f'../data/dataset-v1.1/fake_data/GandCrap/{idx}.png', normalize=True)
            continue

        if data[1] == 1:
            save_image(data[0], f'../data/dataset-v1.1/fake_data/Nitro/{idx}.png', normalize=True)
            continue

        if data[1] == 2:
            save_image(data[0], f'../data/dataset-v1.1/fake_data/Stop/{idx}.png', normalize=True)
            continue

def generate_real_data_64px():
    os.makedirs("../data/dataset-v2/fake_data/Stop/", exist_ok=True)
    os.makedirs("../data/dataset-v2/fake_data/GandCrap/", exist_ok=True)
    os.makedirs("../data/dataset-v2/fake_data/Nitro/", exist_ok=True)
    PATH_TO_GET_DATA = "../data/dataset-v2/data"
    transform = transforms.Compose(
        [transforms.Resize(opt_v2.img_size),
         transforms.CenterCrop(opt_v2.img_size),
         transforms.ToTensor(),
         transforms.Grayscale(),
         transforms.Normalize([0.5], [0.5])]
    )

    data_read = load_image_dataset.read_dataset(path=PATH_TO_GET_DATA, transform_info=transform)
    for idx, data in enumerate(data_read):
        # print(idx, data)
        label_index = data[1]
        if label_index == 0:
            save_image(data[0], f'../data/dataset-v2/fake_data/Nitro/Babuk_{idx}.png', normalize=True)
        elif label_index == 1:
            save_image(data[0], f'../data/dataset-v2/fake_data/Nitro/Cerber_{idx}.png', normalize=True)
        elif label_index == 2:
            save_image(data[0], f'../data/dataset-v2/fake_data/Nitro/Conti_{idx}.png', normalize=True)
        elif label_index == 3:
            save_image(data[0], f'../data/dataset-v2/fake_data/Nitro/BandCrab_{idx}.png', normalize=True)
        elif label_index == 4:
            save_image(data[0], f'../data/dataset-v2/fake_data/Nitro/Locky-Remove_{idx}.png', normalize=True)
        elif label_index == 5:
            save_image(data[0], f'../data/dataset-v2/fake_data/Nitro/Nitro_{idx}.png', normalize=True)
        elif label_index == 6:
            save_image(data[0], f'../data/dataset-v2/fake_data/Nitro/Ryuk-Remove_{idx}.png', normalize=True)
        elif label_index == 7:
            save_image(data[0], f'../data/dataset-v2/fake_data/Nitro/Stop_{idx}.png', normalize=True)
        elif label_index == 8:
            save_image(data[0], f'../data/dataset-v2/fake_data/Nitro/WannaCry_{idx}.png', normalize=True)
        # save_image(data[0], f'../data/dataset-v2/fake_data/Nitro/{idx}.png', normalize=True)
        # if data[1] == 0:
        #     save_image(data[0], f'../data/dataset-v1.1/fake_data/GandCrap/{idx}.png', normalize=True)
        #     continue
        #
        # if data[1] == 1:
        #     save_image(data[0], f'../data/dataset-v1.1/fake_data/Nitro/{idx}.png', normalize=True)
        #     continue
        #
        # if data[1] == 2:
        #     save_image(data[0], f'../data/dataset-v1.1/fake_data/Stop/{idx}.png', normalize=True)
        #     continue

if __name__ == '__main__':
    # generate_fake_data()
    generate_real_data_64px()
