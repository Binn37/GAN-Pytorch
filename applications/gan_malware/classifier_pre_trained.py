import os

import torch
from torch.autograd import Variable
from torch.testing._internal.common_quantization import AverageMeter
from torchvision import transforms

from utils.wandb_helper import init_wandb_classifier, log_loss_value
from utils.load_image_dataset import load_dataset, read_dataset
from config.config_params_V1 import opt
from config.config_core import FloatTensor, cuda
from models.classifier_v32 import MyResNet


def main():
    PATH_TO_GET_DATA = "../data/dataset-v1.1/fake_data"
    os.makedirs(PATH_TO_GET_DATA, exist_ok=True)

    # init model
    classifier_resnet50 = MyResNet(num_classes=opt.n_classes)

    if cuda:
        print("We are using GPU for training models")
        classifier_resnet50.cuda()

    # imbalance datasets
    criterion = torch.nn.CrossEntropyLoss().cuda() if cuda else torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(classifier_resnet50.parameters(), lr=opt.lr)

    # Upload wandb to log data
    init_wandb_classifier(name_project="Resnet50-Mal-V1", entity="walter03", model=classifier_resnet50, opt=opt)

    # load dataset
    transform = transforms.Compose(
        [transforms.ToTensor()]
    )
    data_read = read_dataset(path=PATH_TO_GET_DATA, transform_info=transform)
    dataloader = load_dataset(dataset_read=data_read, batch_size=opt.batch_size, shuffle="true")

    # save loss and display
    total_loss_train, total_loss_test = [], []
    total_acc_train, total_acc_test = [], []

    # init important value
    best_loss = 100
    accs = 0

    print("******** Training ********")
    for epoch in range(opt.n_epochs):
        losses_all_train = AverageMeter()
        for batch_idx, datasets in enumerate(dataloader):
            images, labels = datasets

            images = Variable(FloatTensor(images))
            labels = Variable(FloatTensor(labels))

            embed_feat = classifier_resnet50(images)
            loss = criterion(embed_feat, labels)

            _, preds = torch.max(embed_feat.data, 1)
            acc = torch.sum(preds == labels.data)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            losses_all_train.update(loss)
            accs += acc / labels.shape[0]

        loss_test = test(labels_class, test_data_loader, model, criterion, epoch, total_acc_test, total_loss_test)
        is_best = loss_test < best_loss
        if is_best:
            # Save model
            model_to_save = {
                'state_dict': classifier_resnet50.state_dict(),
            }
            if epoch % args.ckp_freq == 0:
                file_name = os.path.join(exp_dir, "best.pt")
                save_checkpoint(model_to_save, file_name)
            best_loss = loss_test

        total_loss_train.append(losses_all_train.avg)
        total_acc_train.append(100 * accs / len(dataloader))
        log_loss_value(type_value="train", losses_all=losses_all_train, accs=accs, dataloader=dataloader)

        print('Train Epoch: {}\t'
              'Loss: {:.4f} \t'
              'Acc: {:.2f}% \t'.format(
            epoch, losses_all_train.avg, 100 * accs / len(dataloader)))


def test(labels_class, data, model, criterion, epoch, total_acc_test, total_loss_test):
    print("******** Testing ********")
    losses_all_test = AverageMeter()
    accs = 0
    with torch.no_grad():
        model.eval()
        for batch_idx, datasets in enumerate(data):
            images, labels = datasets

            images = Variable(FloatTensor(images))
            labels = Variable(FloatTensor(labels))

            embed_feat = model(images)
            loss = criterion(embed_feat, labels)

            _, preds = torch.max(embed_feat.data, 1)
            acc = torch.sum(preds == labels.data)
            losses_all_test.update(loss)
            accs += acc / labels.shape[0]

        print('Test set: \tLoss: {:.4f}, \tAcc: {:.2f}%'.format(
            losses_all_test.avg, 100 * accs / len(data)))

        total_loss_test.append(losses_all_test.avg)
        total_acc_test.append(100 * accs / len(data))
        log_loss_value(type_value="test", losses_all=losses_all_test, accs=accs, dataloader=data)
        return losses_all_test.avg
