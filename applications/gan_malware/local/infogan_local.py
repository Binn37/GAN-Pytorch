import itertools
import os

import pandas as pd
import torchvision.utils as vutils
import numpy as np
import torch
import torchvision.transforms as transforms
from matplotlib import pyplot as plt
from torch.autograd import Variable

from applications.gan_malware.config.config_core import cuda, FloatTensor, LongTensor
from applications.gan_malware.config.config_params_v3 import Opt
from applications.gan_malware.models.info_gan_v128 import Discriminator, Generator
from applications.gan_malware.utils import load_image_dataset, load_custom_dataset
from applications.gan_malware.utils.helper import weights_init_normal, to_categorical, sample_image_v3_9


def main():
    os.makedirs("images/static/", exist_ok=True)
    os.makedirs("images/varying_c1/", exist_ok=True)
    os.makedirs("images/varying_c2/", exist_ok=True)

    # ----------
    # Setup before training
    # ----------

    # config setting
    opt_v3 = Opt()
    print(vars(opt_v3))

    # Loss functions
    adversarial_loss = torch.nn.MSELoss()
    categorical_loss = torch.nn.CrossEntropyLoss()
    continuous_loss = torch.nn.MSELoss()

    # Loss weights
    lambda_cat = 1
    lambda_con = 0.1

    # Initialize generator and discriminator
    generator = Generator(opt=opt_v3)
    discriminator = Discriminator(opt=opt_v3)

    if cuda:
        print("We are using GPU for training models")
        generator.cuda()
        discriminator.cuda()
        adversarial_loss.cuda()
        categorical_loss.cuda()
        continuous_loss.cuda()

    # Initialize weights
    generator.apply(weights_init_normal)
    discriminator.apply(weights_init_normal)

    # Configure and load data loader
    os.makedirs("../../data/dataset-v1/data", exist_ok=True)
    PATH_TO_GET_DATA = "../../data/dataset-v1/data"
    PATH_TO_GET_PE_FEATURES = "../../data/dataset-v1/classification_ransomware_family.xlsx"
    transform = transforms.Compose(
        [transforms.Resize(opt_v3.img_size_128),
         transforms.CenterCrop(opt_v3.img_size_128),
         transforms.ToTensor(),
         transforms.Grayscale(),
         transforms.Normalize([0.5], [0.5])]
    )

    data_read = load_custom_dataset.ImageLoader(annotations_file=PATH_TO_GET_PE_FEATURES, root=PATH_TO_GET_DATA,
                                                transform=transform)
    data_loader = load_image_dataset.load_dataset(dataset_read=data_read, batch_size=opt_v3.batch_size, shuffle="true")

    unique_dll_vals = data_read.pe_features["DllCharacteristics"].unique()
    unique_dll_vals = pd.get_dummies(unique_dll_vals)
    unique_dll_vals = unique_dll_vals.values

    unique_cs_vals = data_read.pe_features["CheckSum"].unique()
    unique_cs_vals_std = (unique_cs_vals - unique_cs_vals.min(axis=0)) / (
            unique_cs_vals.max(axis=0) - unique_cs_vals.min(axis=0))
    unique_cs_vals = unique_cs_vals_std * (1 - -1) + -1

    # print(f"DllCharacteristics: {unique_dll_vals}")
    # print(f"Describe: {data_read.pe_features['DllCharacteristics'].describe()}")
    # print(f"CheckSum: {unique_cs_vals}")
    # print(f"Describe: {data_read.pe_features['CheckSum'].describe()}")

    # Plot some training images
    # real_batch = next(iter(data_loader))
    # plt.figure(figsize=(8, 8))
    # plt.axis("off")
    # plt.title("Training Images")
    # plt.imshow(
    #     np.transpose(vutils.make_grid(real_batch[0].type(FloatTensor)[:64], padding=2, normalize=True).cpu(),
    #                  (1, 2, 0)))
    # plt.show()

    # Optimizers
    optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt_v3.lr, betas=(opt_v3.b1, opt_v3.b2))
    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt_v3.lr, betas=(opt_v3.b1, opt_v3.b2))
    optimizer_info = torch.optim.Adam(
        itertools.chain(generator.parameters(), discriminator.parameters()), lr=opt_v3.lr, betas=(opt_v3.b1, opt_v3.b2)
    )

    # Static generator inputs for sampling
    static_z = Variable(FloatTensor(np.zeros((opt_v3.n_classes ** 2, opt_v3.latent_dim))))
    static_label = to_categorical(
        y=np.array([num for _ in range(opt_v3.n_classes) for num in range(opt_v3.n_classes)]),
        num_columns=opt_v3.n_classes,
        FloatTensor=FloatTensor
    )
    static_code = Variable(FloatTensor(np.zeros((opt_v3.n_classes ** 2, opt_v3.code_dim))))

    # ----------
    #  Training
    # ----------

    for epoch in range(opt_v3.n_epochs):
        for i, (imgs, labels, pe_code) in enumerate(data_loader):
            batch_size = imgs.shape[0]

            # Adversarial ground truths
            valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)
            fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)

            # Configure input
            real_imgs = Variable(imgs.type(FloatTensor))
            real_labels = to_categorical(labels.numpy(), num_columns=opt_v3.n_classes, FloatTensor=FloatTensor)

            # -----------------
            #  Train Generator
            # -----------------

            optimizer_G.zero_grad()

            # Sample noise and labels as generator input
            z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt_v3.latent_dim))))
            label_input = to_categorical(np.random.randint(0, opt_v3.n_classes, batch_size),
                                         num_columns=opt_v3.n_classes,
                                         FloatTensor=FloatTensor)

            code_dll = np.eye(len(unique_dll_vals[0]))[np.random.choice(unique_dll_vals.shape[0], batch_size)]
            code_cs = np.random.choice(unique_cs_vals, (batch_size, 1))
            code_input = Variable(FloatTensor(np.hstack((code_dll, code_cs))))

            # Generate a batch of images
            gen_imgs = generator(z, label_input, code_input)

            # Loss measures generator's ability to fool the discriminator
            validity, _, _ = discriminator(gen_imgs)
            g_loss = adversarial_loss(validity, valid)

            g_loss.backward()
            optimizer_G.step()

            # ---------------------
            #  Train Discriminator
            # ---------------------

            optimizer_D.zero_grad()

            # Loss for real images
            real_val_pred, real_label_pred, real_latent_code_pred = discriminator(real_imgs)
            d_real_loss = adversarial_loss(real_val_pred, valid)

            # Loss for fake images
            fake_val_pred, fake_label_pred, fake_latent_code_pred = discriminator(gen_imgs.detach())
            d_fake_loss = adversarial_loss(fake_val_pred, fake)

            # Total discriminator loss
            d_loss = (d_real_loss + d_fake_loss) / 2

            d_loss.backward()
            optimizer_D.step()

            # ------------------
            # Information Loss
            # ------------------

            optimizer_info.zero_grad()

            # Sample labels
            sampled_labels = np.random.randint(0, opt_v3.n_classes, batch_size)

            # Ground truth labels
            gt_labels = Variable(LongTensor(sampled_labels), requires_grad=False)

            # Sample noise, labels and code as generator input
            z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt_v3.latent_dim))))
            label_input = to_categorical(sampled_labels, num_columns=opt_v3.n_classes, FloatTensor=FloatTensor)
            # code_input = Variable(FloatTensor(np.random.uniform(-1, 1, (batch_size, opt_v3.code_dim))))
            code_dll = np.eye(len(unique_dll_vals[0]))[np.random.choice(unique_dll_vals.shape[0], batch_size)]
            code_cs = np.random.choice(unique_cs_vals, (batch_size, 1))
            code_input = Variable(FloatTensor(np.hstack((code_dll, code_cs))))

            gen_imgs = generator(z, label_input, code_input)
            _, pred_label, pred_code = discriminator(gen_imgs)

            info_loss = lambda_cat * categorical_loss(pred_label, gt_labels) + lambda_con * continuous_loss(
                pred_code, code_input
            )

            info_loss.backward()
            optimizer_info.step()

            # --------------
            # Log Progress
            # --------------

            print(
                "[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] [info loss: %f]"
                % (epoch, opt_v3.n_epochs, i, len(data_loader), d_loss.item(), g_loss.item(), info_loss.item())
            )
            batches_done = epoch * len(data_loader) + i
            if batches_done % opt_v3.sample_interval == 0:
                sample_image_v3_9(n_row=opt_v3.n_classes, batches_done=batches_done, generator=generator,
                                  static_label=static_label, static_code=static_code, static_z=static_z,
                                  FloatTensor=FloatTensor, latent_dim=opt_v3.latent_dim, unique_cs_vals=unique_cs_vals,
                                  unique_dll_vals=unique_dll_vals)

    torch.save(generator.state_dict(), '../weights/generator_V1.pt')
    torch.save(discriminator.state_dict(), '../weights/discriminator_V1.pt')

    generator_load = Generator(opt=opt_v3)
    discriminator_load = Discriminator(opt=opt_v3)
    generator_load.load_state_dict(torch.load('../weights/generator_V1.pt'))
    generator_load.eval()

    discriminator_load.load_state_dict(torch.load('../weights/discriminator_V1.pt'))
    discriminator_load.eval()


if __name__ == '__main__':
    main()
